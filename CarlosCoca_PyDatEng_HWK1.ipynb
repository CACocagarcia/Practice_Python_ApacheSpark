{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Connecting Google Drive File Directory"
      ],
      "metadata": {
        "id": "73zcoR66Ojmb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DGuMIOJaBtdW",
        "outputId": "43632b37-9afa-4105-e779-efe01ba0c1ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content.drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content.drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/ull\n",
        "!wget -q  https://archive.apache.org/dist/spark/spark-3.2.3/spark-3.2.3-bin-hadoop3.2.tgz"
      ],
      "metadata": {
        "id": "eI_hODzMPg3F"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzip Spark Version\n",
        "!tar xf spark-3.2.3-bin-hadoop3.2.tgz"
      ],
      "metadata": {
        "id": "6bqEvlz6P6wQ"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spark Environment SetUp"
      ],
      "metadata": {
        "id": "7qUe6BYEQuGZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.3-bin-hadoop3.2\"\n",
        "!pip install -q findspark\n",
        "!pip install -q pyspark"
      ],
      "metadata": {
        "id": "_eAe365LQPNm"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Starting Spark Session"
      ],
      "metadata": {
        "id": "2vdv_Kb-Q4mt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession"
      ],
      "metadata": {
        "id": "MDrJUShOQ717"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Starting Spark Session called cap2 that included all available cores\n",
        "cap2 = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "metadata": {
        "id": "hdP0pFZERN3-"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-Creating cap2 Spark Session because we are including appName\n",
        "cap2 = SparkSession.builder.master(\"local[*]\").appName('Task1 Pyspark').getOrCreate()"
      ],
      "metadata": {
        "id": "tLsoadu8RqZ5"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing Libraries to create RDD"
      ],
      "metadata": {
        "id": "lz0rXK1USG15"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession"
      ],
      "metadata": {
        "id": "i_UAYFczSKHM"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We first need to initialize SparkContext\n",
        "sc = cap2.sparkContext"
      ],
      "metadata": {
        "id": "K8RyKmU7SLhh"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating Empty RDD with no Partitions\n",
        "rdd_zero = sc.emptyRDD"
      ],
      "metadata": {
        "id": "q0znkNXhc2rH"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating Empty RDD with 5 Partitions\n",
        "rdd_one = sc.parallelize([],5)\n",
        "\n",
        "# Validating\n",
        "rdd_one.getNumPartitions()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPpoJCKadT2i",
        "outputId": "66cdf0de-37d9-4c43-89fa-e8fcae436308"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a RDD that contains the Prime Numbers from 1-20"
      ],
      "metadata": {
        "id": "Fv0ggfQKfjE_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "number = int(input(\"Input the Prime Range : \"))\n",
        "primenumbers =[] # Creating an empty list that will contain the Prime numbers that will be used to create the RDD\n",
        "\n",
        "for x in range(2,number): # We start from 1 not 0\n",
        "    for i in range(2,x):\n",
        "        if (x%i==0):\n",
        "            break\n",
        "    else:\n",
        "        print(x)\n",
        "        primenumbers.append(x)\n",
        "\n",
        "print(primenumbers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Al-o4nKGfh-U",
        "outputId": "47f2bb23-9c99-4d49-ea78-5a91aeea896d"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input the Prime Range : 20\n",
            "2\n",
            "3\n",
            "5\n",
            "7\n",
            "11\n",
            "13\n",
            "17\n",
            "19\n",
            "[2, 3, 5, 7, 11, 13, 17, 19]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RDD Prime & Validation\n",
        "rdd_prime = sc.parallelize(primenumbers)\n",
        "\n",
        "rdd_prime.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OumhofEBhuQd",
        "outputId": "18cf7c39-abed-4d8e-b3e4-45af79b0b500"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 3, 5, 7, 11, 13, 17, 19]"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a RDD Based Out of the Previous RDD (rdd_prime) but that only\n",
        "contains the Prime Numbers that are greater than 10"
      ],
      "metadata": {
        "id": "_cG9TUSZkZVX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is just a logical test\n",
        "def rddfunprime(pn):\n",
        "\n",
        "  a = 0\n",
        "\n",
        "  for a in range(len(pn)):\n",
        "    if pn[a] > 10:\n",
        "      print(pn[a])\n",
        "      a = a + 1\n",
        "    else:\n",
        "      a = a + 1\n",
        "      pass\n",
        "\n",
        "rddfunprime(primenumbers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBEXyKfWkWpi",
        "outputId": "ca018c54-4675-40a3-8eea-c2ef8a3dda85"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11\n",
            "13\n",
            "17\n",
            "19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd_g10 = rdd_prime.filter(lambda x : x > 10) # New RDD based out of rdd_prime that contains the prime numebers greater than 10\n",
        "rdd_g10.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrPyILsltDAB",
        "outputId": "028b6f03-695c-41d5-fc38-6d1c73ae1661"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[11, 13, 17, 19]"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a RDD Based Out of a File Where All of the File Content is Stored in a Single Record"
      ],
      "metadata": {
        "id": "H0UtE97iujY8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd_file1 = sc.wholeTextFiles(\"/content.drive/MyDrive/Python Data Engineer/el_valor_del_big_data.txt\")"
      ],
      "metadata": {
        "id": "_yACqRVKu_jM"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "  In this block of Code, my objective is to obtain the FilePath of the File that was used to create the\n",
        "  RDD_File1.\n",
        "\n",
        "  The WholeTextFiles() Function saves the data as a List of Tuple: [(String, String)]\n",
        "  ---> Where String1 is the File Path\n",
        "        And String2 is the Entire content of the File\n",
        "\n",
        "  My objective is to only grab String1\n",
        "'''\n",
        "\n",
        "# Grabbing the List of Tuple and save it to the stated variable\n",
        "rdd_list_filepath = rdd_file1.collect()\n",
        "\n",
        "# Grabbing Touple that's positioned in Index0 from the List and save it to stated variable\n",
        "path = rdd_list_filepath[0]\n",
        "\n",
        "# Print String Index0 of the Touple which in this case would be the FilePath\n",
        "print(path[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJRybFlDwOuI",
        "outputId": "cee29899-d7b3-4e56-817c-218b8e8bac10"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "file:/content.drive/MyDrive/Python Data Engineer/el_valor_del_big_data.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a RDD that contains a record per each line of the text file"
      ],
      "metadata": {
        "id": "4eZgmKu32JoU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "  In this block of Code I'm reading and extracting\n",
        "  each line of the file and saving it into the file_data variable\n",
        "\n",
        "  The Type is a List\n",
        "\n",
        "  Using the List I can create a RDD\n",
        "'''\n",
        "file = open(\"/content.drive/MyDrive/Python Data Engineer/el_valor_del_big_data.txt\")\n",
        "file_data = file.readlines()\n",
        "file.close()\n",
        "print(file_data)\n",
        "type(file_data)\n",
        "len(file_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9b9JFf-cxty_",
        "outputId": "7811d11f-b6a4-4fc1-9f64-ce8be92a381e"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['El valor y la realidad de big data\\n', 'En los últimos años, han surgido otras \"dos V\": valor y veracidad. Los datos poseen un valor intrínseco. Sin embargo, no tienen ninguna utilidad hasta que dicho valor se descubre. Resulta igualmente importante: ¿cuál es la veracidad de sus datos y cuánto puede confiar en ellos?\\n', '\\n', 'Hoy en día, el big data se ha convertido en un activo crucial. Piense en algunas de las mayores empresas tecnológicas del mundo. Gran parte del valor que ofrecen procede de sus datos, que analizan constantemente para generar una mayor eficiencia y desarrollar nuevos productos.\\n', '\\n', 'Avances tecnológicos recientes han reducido exponencialmente el coste del almacenamiento y la computación de datos, haciendo que almacenar datos resulte más fácil y barato que nunca. Actualmente, con un mayor volumen de big data más barato y accesible, puede tomar decisiones empresariales más acertadas y precisas.\\n', '\\n', 'Identificar el valor del big data no pasa solo por analizarlo (que es ya una ventaja en sí misma). Se trata de todo un proceso de descubrimiento que requiere que los analistas, usuarios empresariales y ejecutivos se planteen las preguntas correctas, identifiquen patrones, formulen hipótesis informadas y predigan comportamientos.']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd_textline = sc.parallelize(file_data)\n",
        "rdd_textline.collect()\n",
        "rdd_textline.getNumPartitions()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7UT79dyztbN",
        "outputId": "2e3b27cc-bfce-4f49-870f-055e93b68730"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    }
  ]
}